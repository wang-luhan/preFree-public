/**
 * PreFree SpMV Optimized Kernel
 * Auto-generated on {{ timestamp }}
 * 
 * Configuration:
 * - Small matrices (nnz < {{ size_thresholds.small_upper }}): code={{ small_config.code_version }}, P_NNZ={{ small_config.p_nnz }}
 * - Medium matrices (nnz < {{ size_thresholds.medium_upper }}): code={{ medium_config.code_version }}, P_NNZ={{ medium_config.p_nnz }}
 * - Large matrices: code={{ large_config.code_version }}, P_NNZ={{ large_config.p_nnz }}
 */

#include "common.h"

__global__ void pre_start_rowPerTile(const int *__restrict__ row_ptr,
                                     const int m,
                                     int *__restrict__ start_row,
                                     int tileSize)
{
  const int row_id = blockIdx.x * blockDim.x + threadIdx.x;
  if (row_id >= m)
  {
    return;
  }
  const int a = row_ptr[row_id];
  const int b = row_ptr[row_id + 1];
  if (a == b)
  {
    return;
  }
  const int start_block = divup<int>(a, tileSize);
  const int end_block = (b - 1) / tileSize;
  for (int i = start_block; i <= end_block; ++i)
  {
    if (i >= start_block)
    {
      start_row[i] = row_id;
    }
  }
}

// Common reduction functions
template <int TILE_NNZ, int BLOCK_SIZE>
__device__ __forceinline__ void
red_row_block(const int tid_in_block, const int block_id,
              const int tileStartRow,
              const int *__restrict__ row_ptr,
              const valT *__restrict__ smem, valT *__restrict__ y)
{
  constexpr int num_warps = BLOCK_SIZE >> 5;
  __shared__ valT warp_results[num_warps];
  const int reduce_start_idx = max((int)0, row_ptr[tileStartRow] - block_id * TILE_NNZ);
  const int reduce_end_idx = min(TILE_NNZ, row_ptr[tileStartRow + 1] - block_id * TILE_NNZ);

  valT thread_sum = 0;
  for (int j = reduce_start_idx + tid_in_block; j < reduce_end_idx; j += BLOCK_SIZE)
  {
    thread_sum += smem[j];
  }
#pragma unroll
  for (int offset = 16; offset > 0; offset >>= 1)
  {
    thread_sum += __shfl_down_sync(0xffffffff, thread_sum, offset);
  }
  const int warp_id = tid_in_block >> 5;
  const int lane_id = tid_in_block & 31;
  if (lane_id == 0)
  {
    warp_results[warp_id] = thread_sum;
  }
  __syncthreads();

  if (tid_in_block == 0)
  {
    valT final_sum = 0;
    for (int i = 0; i < num_warps; i++)
    {
      final_sum += warp_results[i];
    }
    atomicAdd(y + tileStartRow, final_sum);
  }
}

template <int TILE_NNZ, int BLOCK_SIZE>
__device__ __forceinline__ void red_row_thread(const int tid_in_block, const int block_id,
                                               const int tileStartRow, const int tileEndRow,
                                               const int *__restrict__ row_ptr,
                                               const valT *__restrict__ smem, valT *__restrict__ y)
{
  int reduce_row_id = tileStartRow + tid_in_block;
  int nnz_id_before = block_id * TILE_NNZ;
  for (; reduce_row_id < tileEndRow; reduce_row_id += BLOCK_SIZE)
  {
    valT sum = 0;
    const int reduce_start_idx = max((int)0, row_ptr[reduce_row_id] - nnz_id_before);
    const int reduce_end_idx = min(TILE_NNZ, row_ptr[reduce_row_id + 1] - nnz_id_before);
    for (int i = reduce_start_idx; i < reduce_end_idx; i++)
    {
      sum += smem[i];
    }
    if (reduce_row_id == tileStartRow || reduce_row_id == tileEndRow - 1)
    {
      atomicAdd(y + reduce_row_id, sum);
    }
    else
    {
      y[reduce_row_id] = sum;
    }
  }
}

template <int VEC_SIZE>
__device__ __forceinline__ valT warpReduceSum(valT sum)
{
  if (VEC_SIZE >= 32)
    sum += __shfl_down_sync(0xffffffff, sum, 16);
  if (VEC_SIZE >= 16)
    sum += __shfl_down_sync(0xffffffff, sum, 8);
  if (VEC_SIZE >= 8)
    sum += __shfl_down_sync(0xffffffff, sum, 4);
  if (VEC_SIZE >= 4)
    sum += __shfl_down_sync(0xffffffff, sum, 2);
  if (VEC_SIZE >= 2)
    sum += __shfl_down_sync(0xffffffff, sum, 1);
  return sum;
}

template <int TILE_NNZ, int BLOCK_SIZE, int VECTOR_SIZE>
__device__ __forceinline__ void
red_row_vector(const int tid_in_block, const int block_id,
               const int tileStartRow, const int tileEndRow,
               const int *__restrict__ row_ptr, const valT *__restrict__ smem, valT *__restrict__ y)
{
  const int vec_num = BLOCK_SIZE / VECTOR_SIZE;
  const int vec_id = tid_in_block / VECTOR_SIZE;
  const int tid_in_vec = tid_in_block & (VECTOR_SIZE - 1);

  int reduce_row_id = tileStartRow + vec_id;

  for (; reduce_row_id < tileEndRow; reduce_row_id += vec_num)
  {
    const int reduce_start_idx = max((int)0, row_ptr[reduce_row_id] - block_id * TILE_NNZ);
    const int reduce_end_idx = min((int)TILE_NNZ, row_ptr[reduce_row_id + 1] - block_id * TILE_NNZ);
    valT sum = 0;
    for (int i = reduce_start_idx + tid_in_vec; i < reduce_end_idx; i += VECTOR_SIZE)
    {
      sum += smem[i];
    }
    sum = warpReduceSum<VECTOR_SIZE>(sum);
    if (tid_in_vec == 0)
    {
      if (reduce_row_id == tileStartRow || reduce_row_id == tileEndRow - 1)
      {
        atomicAdd(y + reduce_row_id, sum);
      }
      else
      {
        y[reduce_row_id] = sum;
      }
    }
  }
}

template <int TILE_NNZ, int BLOCK_SIZE, int LONG_ROW_THREADS>
__device__ __forceinline__ void
red_row_long_short_specialized(
    const int n_reduce_rows_num, const int tid_in_block, const int block_id,
    const int tileStartRow, const int tileEndRow,
    const int *__restrict__ row_ptr, const valT *__restrict__ smem, valT *__restrict__ y)
{
  // s_long_row_info: [0] = max_len, [1] = local_idx
  __shared__ int s_long_row_info[2];
  // s_final_sums: 存储所有行的最终计算结果
  __shared__ valT s_final_sums[32];
  // s_warp_sums: 用于长行处理时，存储每个warp的规约结果
  __shared__ valT s_warp_sums[BLOCK_SIZE / 32];

  // 阶段 1: 长行识别
  if (tid_in_block < 32)
  {
    const int lane_id = tid_in_block;
    int my_len = -1;
    if (lane_id < n_reduce_rows_num)
    {
      my_len = row_ptr[tileStartRow + lane_id + 1] - row_ptr[tileStartRow + lane_id];
      s_final_sums[lane_id] = 0.0;
    }

    int max_len = my_len;
    int max_idx = lane_id;

    for (int offset = 16; offset > 0; offset >>= 1)
    {
      int remote_len = __shfl_down_sync(0xFFFFFFFF, max_len, offset);
      int remote_idx = __shfl_down_sync(0xFFFFFFFF, max_idx, offset);
      if (remote_len > max_len)
      {
        max_len = remote_len;
        max_idx = remote_idx;
      }
    }

    if (lane_id == 0)
    {
      s_long_row_info[0] = max_len;
      s_long_row_info[1] = max_idx;
    }
  }

  __syncthreads();

  const int long_row_local_idx = s_long_row_info[1];

  // 团队A: 长行突击队
  if (tid_in_block < LONG_ROW_THREADS)
  {
    if (long_row_local_idx != -1)
    {
      const int reduce_row_id = tileStartRow + long_row_local_idx;
      const int row_start_ptr = row_ptr[reduce_row_id];
      const int row_end_ptr = row_ptr[reduce_row_id + 1];

      const int reduce_start_idx = max((int)0, row_start_ptr - block_id * TILE_NNZ);
      const int reduce_end_idx = min((int)TILE_NNZ, row_end_ptr - block_id * TILE_NNZ);

      valT thread_sum = 0;
      for (int i = reduce_start_idx + tid_in_block; i < reduce_end_idx; i += LONG_ROW_THREADS)
      {
        thread_sum += smem[i];
      }

      for (int offset = 16; offset > 0; offset >>= 1)
      {
        thread_sum += __shfl_down_sync(0xFFFFFFFF, thread_sum, offset);
      }

      const int warp_id = tid_in_block / 32;
      const int lane_id = tid_in_block % 32;
      if (lane_id == 0)
      {
        s_warp_sums[warp_id] = thread_sum;
      }
    }
  }
  // 团队B: 短行清理队
  else
  {
    const int team_tid = tid_in_block - LONG_ROW_THREADS;
    const int team_size = BLOCK_SIZE - LONG_ROW_THREADS;

    for (int i = team_tid; i < n_reduce_rows_num; i += team_size)
    {
      if (i == long_row_local_idx)
        continue;

      const int reduce_row_id = tileStartRow + i;
      const int reduce_start_idx = max((int)0, row_ptr[reduce_row_id] - block_id * TILE_NNZ);
      const int reduce_end_idx = min((int)TILE_NNZ, row_ptr[reduce_row_id + 1] - block_id * TILE_NNZ);

      const int len = reduce_end_idx - reduce_start_idx;
      valT sum = 0;

      switch (len)
      {
      case 1:
        sum = smem[reduce_start_idx];
        break;
      case 2:
        sum = smem[reduce_start_idx] + smem[reduce_start_idx + 1];
        break;
      case 3:
        sum = smem[reduce_start_idx] + smem[reduce_start_idx + 1] + smem[reduce_start_idx + 2];
        break;
      case 4:
        sum = smem[reduce_start_idx] + smem[reduce_start_idx + 1] + smem[reduce_start_idx + 2] + smem[reduce_start_idx + 3];
        break;
      case 0:
        break;
      default:
        for (int k = reduce_start_idx; k < reduce_end_idx; k++)
        {
          sum += smem[k];
        }
        break;
      }
      s_final_sums[i] = sum;
    }
  }

  __syncthreads();

  // 阶段 3: 最终聚合与写回
  if (tid_in_block < 32 && long_row_local_idx != -1)
  {
    const int num_warps = LONG_ROW_THREADS / 32;
    valT final_long_row_sum = (tid_in_block < num_warps) ? s_warp_sums[tid_in_block] : 0.0;

    for (int offset = 16; offset > 0; offset >>= 1)
    {
      final_long_row_sum += __shfl_down_sync(0xFFFFFFFF, final_long_row_sum, offset);
    }

    if (tid_in_block == 0)
    {
      s_final_sums[long_row_local_idx] = final_long_row_sum;
    }
  }

  __syncthreads();

  if (tid_in_block < n_reduce_rows_num)
  {
    const valT final_sum = s_final_sums[tid_in_block];
    if (final_sum == 0.0)
      return;

    const int final_row_id = tileStartRow + tid_in_block;
    if (final_row_id == tileStartRow || final_row_id == tileEndRow - 1)
    {
      atomicAdd(y + final_row_id, final_sum);
    }
    else
    {
      y[final_row_id] = final_sum;
    }
  }
}

__device__ __forceinline__ bool
is_imbalanced_warp(const int tileStartRow, const int n_reduce_rows_num,
                   const int *__restrict__ d_ptr,
                   const int tid_in_block)
{
  const int lane_id = tid_in_block & 31;
  int my_len = 0;
  if (lane_id < n_reduce_rows_num)
  {
    const int my_row_idx = tileStartRow + lane_id;
    my_len = d_ptr[my_row_idx + 1] - d_ptr[my_row_idx];
  }
  int total_nnz = my_len;
  int max_len = my_len;
#pragma unroll
  for (int offset = 16; offset > 0; offset >>= 1)
  {
    total_nnz += __shfl_down_sync(0xFFFFFFFF, total_nnz, offset);
    max_len = max(max_len, __shfl_down_sync(0xFFFFFFFF, max_len, offset));
  }
  bool decision = false;
  if (lane_id == 0)
  {
    const float DOMINANCE_FACTOR = 0.9f;
    if (n_reduce_rows_num > 1 && total_nnz > 0)
    {
      decision = (max_len > total_nnz * DOMINANCE_FACTOR);
    }
  }
  decision = __shfl_sync(0xFFFFFFFF, decision, 0);
  return decision;
}

template <int TILE_NNZ, int BLOCK_SIZE, int VECTOR_SIZE>
__device__ __forceinline__ void
dispatch_reduction_strategy(
    bool use_uneven, int n_reduce_rows_num,
    int tid_in_block, int block_id, int tileStartRow, int tileEndRow,
    const int *__restrict__ d_ptr, const valT *__restrict__ smem, valT *__restrict__ y)
{
  if (use_uneven)
  {
    red_row_long_short_specialized<TILE_NNZ, BLOCK_SIZE, 96>(n_reduce_rows_num, tid_in_block, block_id,
                                                   tileStartRow, tileEndRow, d_ptr, smem, y);
  }
  else
  {
    red_row_vector<TILE_NNZ, BLOCK_SIZE, 4>(tid_in_block, block_id,
                                           tileStartRow, tileEndRow, d_ptr, smem, y);
  }
}

template <int BLOCK_SIZE>
__device__ __forceinline__ unsigned int
calculate_vector_size(int n_reduce_rows_num)
{
  if (n_reduce_rows_num >= BLOCK_SIZE)
  {
    return 2;
  }
  unsigned int avg_threads_per_row = BLOCK_SIZE / n_reduce_rows_num;
  unsigned int highest_power_of_2 = 1 << (31 - __clz(avg_threads_per_row));
  return min(32, max(2, highest_power_of_2));
}

// Small matrix kernel
template <int BLOCK_SIZE>
__global__ void preFreeSpMV_kernel_small(valT *__restrict__ d_val,
                                        int *__restrict__ d_ptr,
                                        int *__restrict__ d_cols,
                                        int rowA,
                                        valT *__restrict__ d_x,
                                        valT *__restrict__ d_y,
                                        int *__restrict__ start_row)
{
  extern __shared__ valT middle_s[];
  const int P_NNZ = {{ small_config.p_nnz }};
  const int last = d_ptr[rowA] - 1;
  const int tileNnz = BLOCK_SIZE * P_NNZ;
  int blockNnzStart = tileNnz * blockIdx.x;

#pragma unroll
  for (int round = 0; round < P_NNZ; round++)
  {
    const int sIdx = threadIdx.x + round * BLOCK_SIZE;
    const int gIdx = min(blockNnzStart + sIdx, last);
    middle_s[sIdx] = d_val[gIdx] * __ldg(&d_x[d_cols[gIdx]]);
  }
  __syncthreads();
  
  const int tileStartRow = min(start_row[blockIdx.x], rowA);
  int tileEndRow = min(start_row[blockIdx.x + 1], rowA);
  tileEndRow = (tileEndRow == 0) ? rowA : tileEndRow;
  if (d_ptr[tileEndRow] % tileNnz != 0 || tileEndRow == tileStartRow)
  {
    tileEndRow = min(tileEndRow + 1, rowA);
  }

  const int n_reduce_rows_num = tileEndRow - tileStartRow;

  // Use best configuration for small matrices
  {% if small_config.code_version == 1 %}
  red_row_thread<tileNnz, BLOCK_SIZE>(threadIdx.x, blockIdx.x,
                                      tileStartRow, tileEndRow,
                                      d_ptr, middle_s, d_y);
  {% elif small_config.code_version == 2 %}
  if (n_reduce_rows_num == 1)
  {
    red_row_block<tileNnz, BLOCK_SIZE>(threadIdx.x, blockIdx.x,
                                       tileStartRow,
                                       d_ptr, middle_s, d_y);
  }
  else
  {
    red_row_thread<tileNnz, BLOCK_SIZE>(threadIdx.x, blockIdx.x,
                                        tileStartRow, tileEndRow,
                                        d_ptr, middle_s, d_y);
  }
  {% elif small_config.code_version == 3 %}
  if (n_reduce_rows_num > BLOCK_SIZE / 4)
  {
    red_row_thread<tileNnz, BLOCK_SIZE>(threadIdx.x, blockIdx.x,
                                        tileStartRow, tileEndRow,
                                        d_ptr, middle_s, d_y);
  }
  else if (n_reduce_rows_num == 1)
  {
    red_row_block<tileNnz, BLOCK_SIZE>(threadIdx.x, blockIdx.x,
                                       tileStartRow,
                                       d_ptr, middle_s, d_y);
  }
  else
  {
    red_row_vector<tileNnz, BLOCK_SIZE, 4>(
        threadIdx.x, blockIdx.x,
        tileStartRow, tileEndRow, d_ptr, middle_s, d_y);
  }
  {% elif small_config.code_version == 4 %}
  if (n_reduce_rows_num > BLOCK_SIZE / 4)
  {
    red_row_thread<tileNnz, BLOCK_SIZE>(threadIdx.x, blockIdx.x,
                                        tileStartRow, tileEndRow,
                                        d_ptr, middle_s, d_y);
  }
  else if (n_reduce_rows_num == 1)
  {
    red_row_block<tileNnz, BLOCK_SIZE>(threadIdx.x, blockIdx.x,
                                       tileStartRow,
                                       d_ptr, middle_s, d_y);
  }
  else
  {
    __shared__ bool use_uneven_path;
    if ((threadIdx.x >> 5) == 0)
    {
      bool decision = is_imbalanced_warp(tileStartRow, n_reduce_rows_num, d_ptr, threadIdx.x);
      if (threadIdx.x == 0)
      {
        use_uneven_path = decision;
      }
    }
    __syncthreads();
    dispatch_reduction_strategy<tileNnz, BLOCK_SIZE, 4>(
        use_uneven_path, n_reduce_rows_num, threadIdx.x, blockIdx.x,
        tileStartRow, tileEndRow, d_ptr, middle_s, d_y);
  }
  {% elif small_config.code_version == 5 %}
  if (n_reduce_rows_num > BLOCK_SIZE)
  {
    red_row_thread<tileNnz, BLOCK_SIZE>(threadIdx.x, blockIdx.x,
                                        tileStartRow, tileEndRow,
                                        d_ptr, middle_s, d_y);
  }
  else if (n_reduce_rows_num == 1)
  {
    red_row_block<tileNnz, BLOCK_SIZE>(threadIdx.x, blockIdx.x,
                                       tileStartRow,
                                       d_ptr, middle_s, d_y);
  }
  else
  {
    const unsigned int vector_size = calculate_vector_size<BLOCK_SIZE>(n_reduce_rows_num);
    switch (vector_size)
    {
    case 32:
      red_row_vector<tileNnz, BLOCK_SIZE, 32>(
          threadIdx.x, blockIdx.x,
          tileStartRow, tileEndRow, d_ptr, middle_s, d_y);
      break;
    case 16:
      red_row_vector<tileNnz, BLOCK_SIZE, 16>(
          threadIdx.x, blockIdx.x,
          tileStartRow, tileEndRow, d_ptr, middle_s, d_y);
      break;
    case 8:
      red_row_vector<tileNnz, BLOCK_SIZE, 8>(
          threadIdx.x, blockIdx.x,
          tileStartRow, tileEndRow, d_ptr, middle_s, d_y);
      break;
    case 4:
      red_row_vector<tileNnz, BLOCK_SIZE, 4>(
          threadIdx.x, blockIdx.x,
          tileStartRow, tileEndRow, d_ptr, middle_s, d_y);
      break;
    case 2:
      red_row_vector<tileNnz, BLOCK_SIZE, 2>(
          threadIdx.x, blockIdx.x,
          tileStartRow, tileEndRow, d_ptr, middle_s, d_y);
      break;
    }
  }
  {% endif %}
}

// Medium matrix kernel
template <int BLOCK_SIZE>
__global__ void preFreeSpMV_kernel_medium(valT *__restrict__ d_val,
                                         int *__restrict__ d_ptr,
                                         int *__restrict__ d_cols,
                                         int rowA,
                                         valT *__restrict__ d_x,
                                         valT *__restrict__ d_y,
                                         int *__restrict__ start_row)
{
  extern __shared__ valT middle_s[];
  const int P_NNZ = {{ medium_config.p_nnz }};
  const int last = d_ptr[rowA] - 1;
  const int tileNnz = BLOCK_SIZE * P_NNZ;
  int blockNnzStart = tileNnz * blockIdx.x;

#pragma unroll
  for (int round = 0; round < P_NNZ; round++)
  {
    const int sIdx = threadIdx.x + round * BLOCK_SIZE;
    const int gIdx = min(blockNnzStart + sIdx, last);
    middle_s[sIdx] = d_val[gIdx] * __ldg(&d_x[d_cols[gIdx]]);
  }
  __syncthreads();
  
  const int tileStartRow = min(start_row[blockIdx.x], rowA);
  int tileEndRow = min(start_row[blockIdx.x + 1], rowA);
  tileEndRow = (tileEndRow == 0) ? rowA : tileEndRow;
  if (d_ptr[tileEndRow] % tileNnz != 0 || tileEndRow == tileStartRow)
  {
    tileEndRow = min(tileEndRow + 1, rowA);
  }

  const int n_reduce_rows_num = tileEndRow - tileStartRow;

  // Use best configuration for medium matrices
  {% if medium_config.code_version == 1 %}
  red_row_thread<tileNnz, BLOCK_SIZE>(threadIdx.x, blockIdx.x,
                                      tileStartRow, tileEndRow,
                                      d_ptr, middle_s, d_y);
  {% elif medium_config.code_version == 2 %}
  if (n_reduce_rows_num == 1)
  {
    red_row_block<tileNnz, BLOCK_SIZE>(threadIdx.x, blockIdx.x,
                                       tileStartRow,
                                       d_ptr, middle_s, d_y);
  }
  else
  {
    red_row_thread<tileNnz, BLOCK_SIZE>(threadIdx.x, blockIdx.x,
                                        tileStartRow, tileEndRow,
                                        d_ptr, middle_s, d_y);
  }
  {% elif medium_config.code_version == 3 %}
  if (n_reduce_rows_num > BLOCK_SIZE / 4)
  {
    red_row_thread<tileNnz, BLOCK_SIZE>(threadIdx.x, blockIdx.x,
                                        tileStartRow, tileEndRow,
                                        d_ptr, middle_s, d_y);
  }
  else if (n_reduce_rows_num == 1)
  {
    red_row_block<tileNnz, BLOCK_SIZE>(threadIdx.x, blockIdx.x,
                                       tileStartRow,
                                       d_ptr, middle_s, d_y);
  }
  else
  {
    red_row_vector<tileNnz, BLOCK_SIZE, 4>(
        threadIdx.x, blockIdx.x,
        tileStartRow, tileEndRow, d_ptr, middle_s, d_y);
  }
  {% elif medium_config.code_version == 4 %}
  if (n_reduce_rows_num > BLOCK_SIZE / 4)
  {
    red_row_thread<tileNnz, BLOCK_SIZE>(threadIdx.x, blockIdx.x,
                                        tileStartRow, tileEndRow,
                                        d_ptr, middle_s, d_y);
  }
  else if (n_reduce_rows_num == 1)
  {
    red_row_block<tileNnz, BLOCK_SIZE>(threadIdx.x, blockIdx.x,
                                       tileStartRow,
                                       d_ptr, middle_s, d_y);
  }
  else
  {
    __shared__ bool use_uneven_path;
    if ((threadIdx.x >> 5) == 0)
    {
      bool decision = is_imbalanced_warp(tileStartRow, n_reduce_rows_num, d_ptr, threadIdx.x);
      if (threadIdx.x == 0)
      {
        use_uneven_path = decision;
      }
    }
    __syncthreads();
    dispatch_reduction_strategy<tileNnz, BLOCK_SIZE, 4>(
        use_uneven_path, n_reduce_rows_num, threadIdx.x, blockIdx.x,
        tileStartRow, tileEndRow, d_ptr, middle_s, d_y);
  }
  {% elif medium_config.code_version == 5 %}
  if (n_reduce_rows_num > BLOCK_SIZE)
  {
    red_row_thread<tileNnz, BLOCK_SIZE>(threadIdx.x, blockIdx.x,
                                        tileStartRow, tileEndRow,
                                        d_ptr, middle_s, d_y);
  }
  else if (n_reduce_rows_num == 1)
  {
    red_row_block<tileNnz, BLOCK_SIZE>(threadIdx.x, blockIdx.x,
                                       tileStartRow,
                                       d_ptr, middle_s, d_y);
  }
  else
  {
    const unsigned int vector_size = calculate_vector_size<BLOCK_SIZE>(n_reduce_rows_num);
    switch (vector_size)
    {
    case 32:
      red_row_vector<tileNnz, BLOCK_SIZE, 32>(
          threadIdx.x, blockIdx.x,
          tileStartRow, tileEndRow, d_ptr, middle_s, d_y);
      break;
    case 16:
      red_row_vector<tileNnz, BLOCK_SIZE, 16>(
          threadIdx.x, blockIdx.x,
          tileStartRow, tileEndRow, d_ptr, middle_s, d_y);
      break;
    case 8:
      red_row_vector<tileNnz, BLOCK_SIZE, 8>(
          threadIdx.x, blockIdx.x,
          tileStartRow, tileEndRow, d_ptr, middle_s, d_y);
      break;
    case 4:
      red_row_vector<tileNnz, BLOCK_SIZE, 4>(
          threadIdx.x, blockIdx.x,
          tileStartRow, tileEndRow, d_ptr, middle_s, d_y);
      break;
    case 2:
      red_row_vector<tileNnz, BLOCK_SIZE, 2>(
          threadIdx.x, blockIdx.x,
          tileStartRow, tileEndRow, d_ptr, middle_s, d_y);
      break;
    }
  }
  {% endif %}
}

// Large matrix kernel
template <int BLOCK_SIZE>
__global__ void preFreeSpMV_kernel_large(valT *__restrict__ d_val,
                                        int *__restrict__ d_ptr,
                                        int *__restrict__ d_cols,
                                        int rowA,
                                        valT *__restrict__ d_x,
                                        valT *__restrict__ d_y,
                                        int *__restrict__ start_row)
{
  extern __shared__ valT middle_s[];
  const int P_NNZ = {% if large_config %}{{ large_config.p_nnz }}{% else %}8{% endif %};
  const int last = d_ptr[rowA] - 1;
  const int tileNnz = BLOCK_SIZE * P_NNZ;
  int blockNnzStart = tileNnz * blockIdx.x;

#pragma unroll
  for (int round = 0; round < P_NNZ; round++)
  {
    const int sIdx = threadIdx.x + round * BLOCK_SIZE;
    const int gIdx = min(blockNnzStart + sIdx, last);
    middle_s[sIdx] = d_val[gIdx] * __ldg(&d_x[d_cols[gIdx]]);
  }
  __syncthreads();
  
  const int tileStartRow = min(start_row[blockIdx.x], rowA);
  int tileEndRow = min(start_row[blockIdx.x + 1], rowA);
  tileEndRow = (tileEndRow == 0) ? rowA : tileEndRow;
  if (d_ptr[tileEndRow] % tileNnz != 0 || tileEndRow == tileStartRow)
  {
    tileEndRow = min(tileEndRow + 1, rowA);
  }

  const int n_reduce_rows_num = tileEndRow - tileStartRow;

  // Use best configuration for large matrices
  {% if large_config and large_config.code_version == 1 %}
  red_row_thread<tileNnz, BLOCK_SIZE>(threadIdx.x, blockIdx.x,
                                      tileStartRow, tileEndRow,
                                      d_ptr, middle_s, d_y);
  {% elif large_config and large_config.code_version == 2 %}
  if (n_reduce_rows_num == 1)
  {
    red_row_block<tileNnz, BLOCK_SIZE>(threadIdx.x, blockIdx.x,
                                       tileStartRow,
                                       d_ptr, middle_s, d_y);
  }
  else
  {
    red_row_thread<tileNnz, BLOCK_SIZE>(threadIdx.x, blockIdx.x,
                                        tileStartRow, tileEndRow,
                                        d_ptr, middle_s, d_y);
  }
  {% elif large_config and large_config.code_version == 3 %}
  if (n_reduce_rows_num > BLOCK_SIZE / 4)
  {
    red_row_thread<tileNnz, BLOCK_SIZE>(threadIdx.x, blockIdx.x,
                                        tileStartRow, tileEndRow,
                                        d_ptr, middle_s, d_y);
  }
  else if (n_reduce_rows_num == 1)
  {
    red_row_block<tileNnz, BLOCK_SIZE>(threadIdx.x, blockIdx.x,
                                       tileStartRow,
                                       d_ptr, middle_s, d_y);
  }
  else
  {
    red_row_vector<tileNnz, BLOCK_SIZE, 4>(
        threadIdx.x, blockIdx.x,
        tileStartRow, tileEndRow, d_ptr, middle_s, d_y);
  }
  {% elif large_config and large_config.code_version == 4 %}
  if (n_reduce_rows_num > BLOCK_SIZE / 4)
  {
    red_row_thread<tileNnz, BLOCK_SIZE>(threadIdx.x, blockIdx.x,
                                        tileStartRow, tileEndRow,
                                        d_ptr, middle_s, d_y);
  }
  else if (n_reduce_rows_num == 1)
  {
    red_row_block<tileNnz, BLOCK_SIZE>(threadIdx.x, blockIdx.x,
                                       tileStartRow,
                                       d_ptr, middle_s, d_y);
  }
  else
  {
    __shared__ bool use_uneven_path;
    if ((threadIdx.x >> 5) == 0)
    {
      bool decision = is_imbalanced_warp(tileStartRow, n_reduce_rows_num, d_ptr, threadIdx.x);
      if (threadIdx.x == 0)
      {
        use_uneven_path = decision;
      }
    }
    __syncthreads();
    dispatch_reduction_strategy<tileNnz, BLOCK_SIZE, 4>(
        use_uneven_path, n_reduce_rows_num, threadIdx.x, blockIdx.x,
        tileStartRow, tileEndRow, d_ptr, middle_s, d_y);
  }
  {% elif large_config and large_config.code_version == 5 %}
  if (n_reduce_rows_num > BLOCK_SIZE)
  {
    red_row_thread<tileNnz, BLOCK_SIZE>(threadIdx.x, blockIdx.x,
                                        tileStartRow, tileEndRow,
                                        d_ptr, middle_s, d_y);
  }
  else if (n_reduce_rows_num == 1)
  {
    red_row_block<tileNnz, BLOCK_SIZE>(threadIdx.x, blockIdx.x,
                                       tileStartRow,
                                       d_ptr, middle_s, d_y);
  }
  else
  {
    const unsigned int vector_size = calculate_vector_size<BLOCK_SIZE>(n_reduce_rows_num);
    switch (vector_size)
    {
    case 32:
      red_row_vector<tileNnz, BLOCK_SIZE, 32>(
          threadIdx.x, blockIdx.x,
          tileStartRow, tileEndRow, d_ptr, middle_s, d_y);
      break;
    case 16:
      red_row_vector<tileNnz, BLOCK_SIZE, 16>(
          threadIdx.x, blockIdx.x,
          tileStartRow, tileEndRow, d_ptr, middle_s, d_y);
      break;
    case 8:
      red_row_vector<tileNnz, BLOCK_SIZE, 8>(
          threadIdx.x, blockIdx.x,
          tileStartRow, tileEndRow, d_ptr, middle_s, d_y);
      break;
    case 4:
      red_row_vector<tileNnz, BLOCK_SIZE, 4>(
          threadIdx.x, blockIdx.x,
          tileStartRow, tileEndRow, d_ptr, middle_s, d_y);
      break;
    case 2:
      red_row_vector<tileNnz, BLOCK_SIZE, 2>(
          threadIdx.x, blockIdx.x,
          tileStartRow, tileEndRow, d_ptr, middle_s, d_y);
      break;
    }
  }
  {% else %}
  // Default configuration when large_config is not available
  red_row_thread<tileNnz, BLOCK_SIZE>(threadIdx.x, blockIdx.x,
                                      tileStartRow, tileEndRow,
                                      d_ptr, middle_s, d_y);
  {% endif %}
}

// Main entry point with automatic kernel selection
void preFreeSpMV(valT *csrVal, int *csrRowPtr, int *csrColInd,
                 valT *X_val, valT *Y_val, int rowA, int colA, int nnzA,
                 double *cdTime, double *cdPre)
{
  CudaTimer timer;
  valT *d_vecY_accu, *d_vecY_perf, *d_vecX, *d_val;
  int *d_indices, *d_ptr;

  cudaMalloc(&d_vecY_accu, sizeof(valT) * rowA);
  cudaMalloc(&d_vecY_perf, sizeof(valT) * rowA);
  cudaMalloc(&d_vecX, sizeof(valT) * colA);
  cudaMalloc(&d_val, sizeof(valT) * nnzA);
  cudaMalloc(&d_indices, sizeof(int) * nnzA);
  cudaMalloc(&d_ptr, sizeof(int) * (rowA + 2));

  cudaMemcpy(d_val, csrVal, sizeof(valT) * nnzA, cudaMemcpyHostToDevice);
  cudaMemcpy(d_indices, csrColInd, sizeof(int) * nnzA, cudaMemcpyHostToDevice);
  cudaMemcpy(d_ptr, csrRowPtr, sizeof(int) * (rowA + 2), cudaMemcpyHostToDevice);
  cudaMemcpy(d_vecX, X_val, sizeof(valT) * colA, cudaMemcpyHostToDevice);

  const int BLOCK_SIZE = 128;
  
  // Determine matrix size class and P_NNZ
  int P_NNZ;
  enum SizeClass { SMALL, MEDIUM, LARGE } size_class;
  
  if (nnzA < {{ size_thresholds.small_upper }}) {
    size_class = SMALL;
    P_NNZ = {{ small_config.p_nnz }};
  } else if (nnzA < {{ size_thresholds.medium_upper }}) {
    size_class = MEDIUM;
    P_NNZ = {{ medium_config.p_nnz }};
  } else {
    size_class = LARGE;
    P_NNZ = {% if large_config %}{{ large_config.p_nnz }}{% else %}8{% endif %};
  }
  
  const int WORK_BLOCKS = nnzA / (P_NNZ * BLOCK_SIZE) + ((nnzA % (P_NNZ * BLOCK_SIZE) == 0) ? 0 : 1);
  const int start_row_len = WORK_BLOCKS + 1;

  int *start_row_accu, *start_row_perf;
  cudaMalloc(&start_row_accu, sizeof(int) * start_row_len);
  cudaMalloc(&start_row_perf, sizeof(int) * start_row_len);
  cudaMemset(start_row_accu, 0, sizeof(int) * start_row_len);
  cudaMemset(start_row_perf, 0, sizeof(int) * start_row_len);

  int tileNnz = BLOCK_SIZE * P_NNZ;

  pre_start_rowPerTile<<<divup<uint32_t>(rowA + 1, 128), 128>>>(d_ptr, rowA,
                                                                start_row_accu,
                                                                tileNnz);
  cudaDeviceSynchronize();

  int warm_iter = 200;
  int test_iter = 4000;
  
  // Pre-processing timing
  for (int i = 0; i < warm_iter; ++i)
  {
    pre_start_rowPerTile<<<divup<uint32_t>(rowA + 1, 128), 128>>>(d_ptr, rowA,
                                                                  start_row_perf,
                                                                  tileNnz);
  }
  cudaDeviceSynchronize();

  timer.start();
  for (int i = 0; i < test_iter; ++i)
  {
    pre_start_rowPerTile<<<divup<uint32_t>(rowA + 1, 128), 128>>>(d_ptr, rowA,
                                                                  start_row_perf,
                                                                  tileNnz);
  }
  float pre_total_time_ms = timer.stop();
  *cdPre = (double)pre_total_time_ms / test_iter;

  cudaError_t err = cudaGetLastError();
  if (err != cudaSuccess)
  {
    printf("pre_start_rowPerTile kernel launch failed: %s\n", cudaGetErrorString(err));
  }
  
  const int sharedMemSize = tileNnz * sizeof(valT);
  cudaMemset(d_vecY_perf, 0.0, sizeof(valT) * rowA);

  // Warm-up runs
  for (int i = 0; i < warm_iter; ++i)
  {
    switch (size_class) {
      case SMALL:
        preFreeSpMV_kernel_small<BLOCK_SIZE><<<WORK_BLOCKS, BLOCK_SIZE, sharedMemSize>>>(
            d_val, d_ptr, d_indices, rowA, d_vecX, d_vecY_perf, start_row_accu);
        break;
      case MEDIUM:
        preFreeSpMV_kernel_medium<BLOCK_SIZE><<<WORK_BLOCKS, BLOCK_SIZE, sharedMemSize>>>(
            d_val, d_ptr, d_indices, rowA, d_vecX, d_vecY_perf, start_row_accu);
        break;
      case LARGE:
        preFreeSpMV_kernel_large<BLOCK_SIZE><<<WORK_BLOCKS, BLOCK_SIZE, sharedMemSize>>>(
            d_val, d_ptr, d_indices, rowA, d_vecX, d_vecY_perf, start_row_accu);
        break;
    }
  }
  cudaDeviceSynchronize();
  
  // Performance measurement
  timer.start();
  for (int i = 0; i < test_iter; ++i)
  {
    switch (size_class) {
      case SMALL:
        preFreeSpMV_kernel_small<BLOCK_SIZE><<<WORK_BLOCKS, BLOCK_SIZE, sharedMemSize>>>(
            d_val, d_ptr, d_indices, rowA, d_vecX, d_vecY_perf, start_row_accu);
        break;
      case MEDIUM:
        preFreeSpMV_kernel_medium<BLOCK_SIZE><<<WORK_BLOCKS, BLOCK_SIZE, sharedMemSize>>>(
            d_val, d_ptr, d_indices, rowA, d_vecX, d_vecY_perf, start_row_accu);
        break;
      case LARGE:
        preFreeSpMV_kernel_large<BLOCK_SIZE><<<WORK_BLOCKS, BLOCK_SIZE, sharedMemSize>>>(
            d_val, d_ptr, d_indices, rowA, d_vecX, d_vecY_perf, start_row_accu);
        break;
    }
  }
  float total_loop_time_ms = timer.stop();

  // Final computation for accuracy check
  cudaMemset(d_vecY_accu, 0.0, sizeof(valT) * rowA);
  switch (size_class) {
    case SMALL:
      preFreeSpMV_kernel_small<BLOCK_SIZE><<<WORK_BLOCKS, BLOCK_SIZE, sharedMemSize>>>(
          d_val, d_ptr, d_indices, rowA, d_vecX, d_vecY_accu, start_row_accu);
      break;
    case MEDIUM:
      preFreeSpMV_kernel_medium<BLOCK_SIZE><<<WORK_BLOCKS, BLOCK_SIZE, sharedMemSize>>>(
          d_val, d_ptr, d_indices, rowA, d_vecX, d_vecY_accu, start_row_accu);
      break;
    case LARGE:
      preFreeSpMV_kernel_large<BLOCK_SIZE><<<WORK_BLOCKS, BLOCK_SIZE, sharedMemSize>>>(
          d_val, d_ptr, d_indices, rowA, d_vecX, d_vecY_accu, start_row_accu);
      break;
  }

  cudaDeviceSynchronize();
  CUDA_CHECK_ERROR(cudaGetLastError());

  *cdTime = (double)total_loop_time_ms / test_iter;
  double cd_gflops = (double)((long)nnzA * 2) / (total_loop_time_ms * 1e6);

  CUDA_CHECK_ERROR(cudaMemcpy(Y_val, d_vecY_accu, sizeof(valT) * rowA, cudaMemcpyDeviceToHost));
  
  cudaFree(d_vecY_perf);
  cudaFree(d_vecY_accu);
  cudaFree(d_vecX);
  cudaFree(d_val);
  cudaFree(d_indices);
  cudaFree(d_ptr);
  cudaFree(start_row_accu);
  cudaFree(start_row_perf);
}